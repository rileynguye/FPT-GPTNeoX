{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rileynguye/FPT-GPTNeoX/blob/main/FPT_with_GPTNeoX_backbone_for_Few_shot_learning_Time_Series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required packages (required version)"
      ],
      "metadata": {
        "id": "oxlqVUlQgFgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install absl-py==1.2.0 einops==0.4.1 h5py==3.7.0 keopscore==2.1 opt-einsum==3.3.0 pandas==1.4.2 PyWavelets==1.4.1 scikit-image==0.19.3 scikit-learn==1.0.2 scipy==1.7.3 statsmodels==0.13.2 sympy==1.11.1 torch==1.8.1 transformers==4.30.1\n"
      ],
      "metadata": {
        "id": "Fnz0k6ZAgB7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "General import"
      ],
      "metadata": {
        "id": "wiJ6bEyMz0zy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsDR_0nhYiwz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import weight_norm\n",
        "import math\n",
        "from transformers import GPTNeoXConfig, GPTNeoXModel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cVnGPQyGf6H",
        "outputId": "fc09eac8-077e-44ea-f8ec-f76a9e47f7ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "hEoaKY4gMqCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import random\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "DZqKpNNVQMsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports for preprocessing data"
      ],
      "metadata": {
        "id": "SGzkXIi_z6jC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "\n",
        "from datetime import datetime\n",
        "from distutils.util import strtobool\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from pandas.tseries import offsets\n",
        "from pandas.tseries.frequencies import to_offset\n",
        "\n"
      ],
      "metadata": {
        "id": "beu6ItGaziGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports for loading data"
      ],
      "metadata": {
        "id": "pIpZykO3bXhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "XdrSwGrNbWIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define embedding layers"
      ],
      "metadata": {
        "id": "UBH11POO-vQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe.require_grad = False\n",
        "\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "SNEJDYwF-Nl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
        "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
        "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        # print(\"x.shape = {}\".format(x.shape))\n",
        "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "pQNB6tfY-5GN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FixedEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model):\n",
        "        super(FixedEmbedding, self).__init__()\n",
        "\n",
        "        w = torch.zeros(c_in, d_model).float()\n",
        "        w.require_grad = False\n",
        "\n",
        "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
        "\n",
        "        w[:, 0::2] = torch.sin(position * div_term)\n",
        "        w[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.emb = nn.Embedding(c_in, d_model)\n",
        "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.emb(x).detach()\n"
      ],
      "metadata": {
        "id": "fr2yOI0LApd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
        "        super(TemporalEmbedding, self).__init__()\n",
        "\n",
        "        minute_size = 4\n",
        "        hour_size = 24\n",
        "        weekday_size = 7\n",
        "        day_size = 32\n",
        "        month_size = 13\n",
        "\n",
        "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
        "        if freq == 't':\n",
        "            self.minute_embed = Embed(minute_size, d_model)\n",
        "        self.hour_embed = Embed(hour_size, d_model)\n",
        "        self.weekday_embed = Embed(weekday_size, d_model)\n",
        "        self.day_embed = Embed(day_size, d_model)\n",
        "        self.month_embed = Embed(month_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.long()\n",
        "\n",
        "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(self, 'minute_embed') else 0.\n",
        "        hour_x = self.hour_embed(x[:, :, 3])\n",
        "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
        "        day_x = self.day_embed(x[:, :, 1])\n",
        "        month_x = self.month_embed(x[:, :, 0])\n",
        "\n",
        "        return hour_x + weekday_x + day_x + month_x + minute_x\n"
      ],
      "metadata": {
        "id": "ySPAwJwYAthF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeFeatureEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
        "        super(TimeFeatureEmbedding, self).__init__()\n",
        "\n",
        "        freq_map = {'h': 4, 't': 5, 's': 6, 'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n",
        "        d_inp = freq_map[freq]\n",
        "        self.embed = nn.Linear(d_inp, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)"
      ],
      "metadata": {
        "id": "UQ2aINAuBb63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataEmbedding(nn.Module):\n",
        "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
        "        super(DataEmbedding, self).__init__()\n",
        "\n",
        "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
        "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
        "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
        "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, x_mark):\n",
        "        x = self.value_embedding(x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "KTcJxa_AB2iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataEmbedding_wo_pos(nn.Module):\n",
        "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
        "        super(DataEmbedding_wo_pos, self).__init__()\n",
        "\n",
        "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
        "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
        "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
        "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x, x_mark):\n",
        "        x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "BAC8poLjCGHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataEmbedding_wo_time(nn.Module):\n",
        "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
        "        super(DataEmbedding_wo_time, self).__init__()\n",
        "\n",
        "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.value_embedding(x) + self.position_embedding(x)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "2h8Frn2fC5Wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the model"
      ],
      "metadata": {
        "id": "fL8RUzkAgPXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTNeoX4TS(nn.Module):\n",
        "\n",
        "    def __init__(self, configs, device):\n",
        "        super(GPTNeoX4TS, self).__init__()\n",
        "        self.is_gpt = configs.is_gpt\n",
        "        self.patch_size = configs.patch_size\n",
        "        self.pretrain = configs.pretrain\n",
        "        self.stride = configs.stride\n",
        "        self.gpt_layers = 6\n",
        "        self.patch_num = (configs.seq_len - self.patch_size) // self.stride + 1\n",
        "\n",
        "\n",
        "\n",
        "        self.padding_patch_layer = nn.ReplicationPad1d((0, self.stride))\n",
        "        self.patch_num += 1\n",
        "\n",
        "        if configs.is_gpt:\n",
        "            if configs.pretrain:\n",
        "                self.gpt_neox = GPTNeoXModel.from_pretrained(\"EleutherAI/gpt-neox-20b\", output_attentions=True, output_hidden_states=True)\n",
        "            else:\n",
        "                print(\"------------------no pretrain------------------\")\n",
        "                self.gpt_neox = GPTNeoXModel(GPTNeoXConfig())\n",
        "            self.gpt_neox.h = self.gpt_neox.h[:configs.gpt_layers]\n",
        "            print(\"gpt_neox = {}\".format(self.gpt_neox))\n",
        "\n",
        "        self.in_layer = nn.Linear(configs.patch_size, configs.d_model)\n",
        "        self.out_layer = nn.Linear(configs.d_model * self.patch_num, configs.pred_len)\n",
        "#Frozen block definition\n",
        "        if configs.freeze and configs.pretrain:\n",
        "            for i, (name, param) in enumerate(self.gpt_neox.named_parameters()):\n",
        "                if 'ln' in name or 'wpe' in name:\n",
        "                    param.requires_grad = True\n",
        "                else:\n",
        "                    param.requires_grad = False\n",
        "\n",
        "        for layer in (self.gpt_neox, self.in_layer, self.out_layer):\n",
        "            layer.to(device=device)\n",
        "            layer.train()\n",
        "\n",
        "        self.cnt = 0\n",
        "\n",
        "\n",
        "    def forward(self, x, itr):\n",
        "        B, L, M = x.shape\n",
        "\n",
        "        means = x.mean(1, keepdim=True).detach()\n",
        "        x = x - means\n",
        "        stdev = torch.sqrt(torch.var(x, dim=1, keepdim=True, unbiased=False)+ 1e-5).detach()\n",
        "        x /= stdev\n",
        "\n",
        "        x = rearrange(x, 'b l m -> b m l')\n",
        "\n",
        "        x = self.padding_patch_layer(x)\n",
        "        x = x.unfold(dimension=-1, size=self.patch_size, step=self.stride)\n",
        "        x = rearrange(x, 'b m n p -> (b m) n p')\n",
        "\n",
        "        outputs = self.in_layer(x)\n",
        "        if self.is_gpt:\n",
        "            outputs = self.gpt_neox(inputs_embeds=outputs).last_hidden_state\n",
        "\n",
        "        outputs = self.out_layer(outputs.reshape(B*M, -1))\n",
        "        outputs = rearrange(outputs, '(b m) l -> b l m', b=B)\n",
        "\n",
        "        outputs = outputs * stdev\n",
        "        outputs = outputs + means\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "BsMmICZUDUDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define some tools for data processing"
      ],
      "metadata": {
        "id": "mbRjPUVE2Rpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.switch_backend('agg')"
      ],
      "metadata": {
        "id": "W59snr_wfwHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    # lr = args.learning_rate * (0.2 ** (epoch // 2))\n",
        "    # if args.decay_fac is None:\n",
        "    #     args.decay_fac = 0.5\n",
        "    # if args.lradj == 'type1':\n",
        "    #     lr_adjust = {epoch: args.learning_rate * (args.decay_fac ** ((epoch - 1) // 1))}\n",
        "    # elif args.lradj == 'type2':\n",
        "    #     lr_adjust = {\n",
        "    #         2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
        "    #         10: 5e-7, 15: 1e-7, 20: 5e-8\n",
        "    #     }\n",
        "    if args.lradj =='type1':\n",
        "        lr_adjust = {epoch: args.learning_rate if epoch < 3 else args.learning_rate * (0.9 ** ((epoch - 3) // 1))}\n",
        "    elif args.lradj =='type2':\n",
        "        lr_adjust = {epoch: args.learning_rate * (args.decay_fac ** ((epoch - 1) // 1))}\n",
        "    elif args.lradj =='type4':\n",
        "        lr_adjust = {epoch: args.learning_rate * (args.decay_fac ** ((epoch) // 1))}\n",
        "    else:\n",
        "        args.learning_rate = 1e-4\n",
        "        lr_adjust = {epoch: args.learning_rate if epoch < 3 else args.learning_rate * (0.9 ** ((epoch - 3) // 1))}\n",
        "    print(\"lr_adjust = {}\".format(lr_adjust))\n",
        "    if epoch in lr_adjust.keys():\n",
        "        lr = lr_adjust[epoch]\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        print('Updating learning rate to {}'.format(lr))\n"
      ],
      "metadata": {
        "id": "N44VHDyE2msv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model, path):\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, path)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model, path)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model, path):\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
        "        self.val_loss_min = val_loss"
      ],
      "metadata": {
        "id": "wo1_jBjl2t9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dotdict(dict):\n",
        "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "    __getattr__ = dict.get\n",
        "    __setattr__ = dict.__setitem__\n",
        "    __delattr__ = dict.__delitem__"
      ],
      "metadata": {
        "id": "k0LIz89-3HIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StandardScaler():\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def transform(self, data):\n",
        "        return (data - self.mean) / self.std\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return (data * self.std) + self.mean\n"
      ],
      "metadata": {
        "id": "Z-I5e5Cy3ItG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_tsf_to_dataframe(\n",
        "    full_file_path_and_name,\n",
        "    replace_missing_vals_with=\"NaN\",\n",
        "    value_column_name=\"series_value\",\n",
        "):\n",
        "    col_names = []\n",
        "    col_types = []\n",
        "    all_data = {}\n",
        "    line_count = 0\n",
        "    frequency = None\n",
        "    forecast_horizon = None\n",
        "    contain_missing_values = None\n",
        "    contain_equal_length = None\n",
        "    found_data_tag = False\n",
        "    found_data_section = False\n",
        "    started_reading_data_section = False\n",
        "\n",
        "    with open(full_file_path_and_name, \"r\", encoding=\"cp1252\") as file:\n",
        "        for line in file:\n",
        "            # Strip white space from start/end of line\n",
        "            line = line.strip()\n",
        "\n",
        "            if line:\n",
        "                if line.startswith(\"@\"):  # Read meta-data\n",
        "                    if not line.startswith(\"@data\"):\n",
        "                        line_content = line.split(\" \")\n",
        "                        if line.startswith(\"@attribute\"):\n",
        "                            if (\n",
        "                                len(line_content) != 3\n",
        "                            ):  # Attributes have both name and type\n",
        "                                raise Exception(\"Invalid meta-data specification.\")\n",
        "\n",
        "                            col_names.append(line_content[1])\n",
        "                            col_types.append(line_content[2])\n",
        "                        else:\n",
        "                            if (\n",
        "                                len(line_content) != 2\n",
        "                            ):  # Other meta-data have only values\n",
        "                                raise Exception(\"Invalid meta-data specification.\")\n",
        "\n",
        "                            if line.startswith(\"@frequency\"):\n",
        "                                frequency = line_content[1]\n",
        "                            elif line.startswith(\"@horizon\"):\n",
        "                                forecast_horizon = int(line_content[1])\n",
        "                            elif line.startswith(\"@missing\"):\n",
        "                                contain_missing_values = bool(\n",
        "                                    strtobool(line_content[1])\n",
        "                                )\n",
        "                            elif line.startswith(\"@equallength\"):\n",
        "                                contain_equal_length = bool(strtobool(line_content[1]))\n",
        "\n",
        "                    else:\n",
        "                        if len(col_names) == 0:\n",
        "                            raise Exception(\n",
        "                                \"Missing attribute section. Attribute section must come before data.\"\n",
        "                            )\n",
        "\n",
        "                        found_data_tag = True\n",
        "                elif not line.startswith(\"#\"):\n",
        "                    if len(col_names) == 0:\n",
        "                        raise Exception(\n",
        "                            \"Missing attribute section. Attribute section must come before data.\"\n",
        "                        )\n",
        "                    elif not found_data_tag:\n",
        "                        raise Exception(\"Missing @data tag.\")\n",
        "                    else:\n",
        "                        if not started_reading_data_section:\n",
        "                            started_reading_data_section = True\n",
        "                            found_data_section = True\n",
        "                            all_series = []\n",
        "\n",
        "                            for col in col_names:\n",
        "                                all_data[col] = []\n",
        "\n",
        "                        full_info = line.split(\":\")\n",
        "\n",
        "                        if len(full_info) != (len(col_names) + 1):\n",
        "                            raise Exception(\"Missing attributes/values in series.\")\n",
        "\n",
        "                        series = full_info[len(full_info) - 1]\n",
        "                        series = series.split(\",\")\n",
        "\n",
        "                        if len(series) == 0:\n",
        "                            raise Exception(\n",
        "                                \"A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series. Missing values should be indicated with ? symbol\"\n",
        "                            )\n",
        "\n",
        "                        numeric_series = []\n",
        "\n",
        "                        for val in series:\n",
        "                            if val == \"?\":\n",
        "                                numeric_series.append(replace_missing_vals_with)\n",
        "                            else:\n",
        "                                numeric_series.append(float(val))\n",
        "\n",
        "                        if numeric_series.count(replace_missing_vals_with) == len(\n",
        "                            numeric_series\n",
        "                        ):\n",
        "                            raise Exception(\n",
        "                                \"All series values are missing. A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series.\"\n",
        "                            )\n",
        "\n",
        "                        all_series.append(pd.Series(numeric_series).array)\n",
        "\n",
        "                        for i in range(len(col_names)):\n",
        "                            att_val = None\n",
        "                            if col_types[i] == \"numeric\":\n",
        "                                att_val = int(full_info[i])\n",
        "                            elif col_types[i] == \"string\":\n",
        "                                att_val = str(full_info[i])\n",
        "                            elif col_types[i] == \"date\":\n",
        "                                att_val = datetime.strptime(\n",
        "                                    full_info[i], \"%Y-%m-%d %H-%M-%S\"\n",
        "                                )\n",
        "                            else:\n",
        "                                raise Exception(\n",
        "                                    \"Invalid attribute type.\"\n",
        "                                )  # Currently, the code supports only numeric, string and date types. Extend this as required.\n",
        "\n",
        "                            if att_val is None:\n",
        "                                raise Exception(\"Invalid attribute value.\")\n",
        "                            else:\n",
        "                                all_data[col_names[i]].append(att_val)\n",
        "\n",
        "                line_count = line_count + 1\n",
        "\n",
        "        if line_count == 0:\n",
        "            raise Exception(\"Empty file.\")\n",
        "        if len(col_names) == 0:\n",
        "            raise Exception(\"Missing attribute section.\")\n",
        "        if not found_data_section:\n",
        "            raise Exception(\"Missing series information under data section.\")\n",
        "\n",
        "        all_data[value_column_name] = all_series\n",
        "        loaded_data = pd.DataFrame(all_data)\n",
        "\n",
        "        return (\n",
        "            loaded_data,\n",
        "            frequency,\n",
        "            forecast_horizon,\n",
        "            contain_missing_values,\n",
        "            contain_equal_length,\n",
        "        )"
      ],
      "metadata": {
        "id": "dSf9VKzi3oFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vali(model, vali_data, vali_loader, criterion, args, device, itr):\n",
        "    total_loss = []\n",
        "\n",
        "        model.in_layer.eval()\n",
        "        model.out_layer.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in tqdm(enumerate(vali_loader)):\n",
        "            batch_x = batch_x.float().to(device)\n",
        "            batch_y = batch_y.float()\n",
        "\n",
        "            batch_x_mark = batch_x_mark.float().to(device)\n",
        "            batch_y_mark = batch_y_mark.float().to(device)\n",
        "\n",
        "            outputs = model(batch_x, itr)\n",
        "\n",
        "            # encoder - decoder\n",
        "            outputs = outputs[:, -args.pred_len:, :]\n",
        "            batch_y = batch_y[:, -args.pred_len:, :].to(device)\n",
        "\n",
        "            pred = outputs.detach().cpu()\n",
        "            true = batch_y.detach().cpu()\n",
        "\n",
        "            loss = criterion(pred, true)\n",
        "\n",
        "            total_loss.append(loss)\n",
        "    total_loss = np.average(total_loss)\n",
        "\n",
        "        model.in_layer.train()\n",
        "        model.out_layer.train()\n",
        "    return total_loss\n",
        "\n",
        "def MASE(x, freq, pred, true):\n",
        "    masep = np.mean(np.abs(x[:, freq:] - x[:, :-freq]))\n",
        "    return np.mean(np.abs(pred - true) / (masep + 1e-8))\n",
        "\n",
        "def test(model, test_data, test_loader, args, device, itr):\n",
        "    preds = []\n",
        "    trues = []\n",
        "    # mases = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in tqdm(enumerate(test_loader)):\n",
        "\n",
        "            # outputs_np = batch_x.cpu().numpy()\n",
        "            # np.save(\"emb_test/ETTh2_192_test_input_itr{}_{}.npy\".format(itr, i), outputs_np)\n",
        "            # outputs_np = batch_y.cpu().numpy()\n",
        "            # np.save(\"emb_test/ETTh2_192_test_true_itr{}_{}.npy\".format(itr, i), outputs_np)\n",
        "\n",
        "            batch_x = batch_x.float().to(device)\n",
        "            batch_y = batch_y.float()\n",
        "\n",
        "            outputs = model(batch_x[:, -args.seq_len:, :], itr)\n",
        "\n",
        "            # encoder - decoder\n",
        "            outputs = outputs[:, -args.pred_len:, :]\n",
        "            batch_y = batch_y[:, -args.pred_len:, :].to(device)\n",
        "\n",
        "            pred = outputs.detach().cpu().numpy()\n",
        "            true = batch_y.detach().cpu().numpy()\n",
        "\n",
        "            preds.append(pred)\n",
        "            trues.append(true)\n",
        "\n",
        "    preds = np.array(preds)\n",
        "    trues = np.array(trues)\n",
        "    # mases = np.mean(np.array(mases))\n",
        "    print('test shape:', preds.shape, trues.shape)\n",
        "    preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n",
        "    trues = trues.reshape(-1, trues.shape[-2], trues.shape[-1])\n",
        "    print('test shape:', preds.shape, trues.shape)\n",
        "\n",
        "    mae, mse, rmse, mape, mspe, smape, nd = metric(preds, trues)\n",
        "    # print('mae:{:.4f}, mse:{:.4f}, rmse:{:.4f}, smape:{:.4f}, mases:{:.4f}'.format(mae, mse, rmse, smape, mases))\n",
        "    print('mae:{:.4f}, mse:{:.4f}, rmse:{:.4f}, smape:{:.4f}'.format(mae, mse, rmse, smape))\n",
        "\n",
        "    return mse, mae"
      ],
      "metadata": {
        "id": "IpoYg_Qg3_E9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert multivariate time features to univariate time features"
      ],
      "metadata": {
        "id": "nYPsohkV4PA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeFeature:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        pass\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + \"()\""
      ],
      "metadata": {
        "id": "bVJyGIQe4Gqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SecondOfMinute(TimeFeature):\n",
        "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.second / 59.0 - 0.5"
      ],
      "metadata": {
        "id": "-H2EOfaS4cTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MinuteOfHour(TimeFeature):\n",
        "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.minute / 59.0 - 0.5\n"
      ],
      "metadata": {
        "id": "9_V9R9Hx4jEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HourOfDay(TimeFeature):\n",
        "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.hour / 23.0 - 0.5\n",
        "\n",
        "\n",
        "class DayOfWeek(TimeFeature):\n",
        "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return index.dayofweek / 6.0 - 0.5\n",
        "\n",
        "\n",
        "class DayOfMonth(TimeFeature):\n",
        "    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.day - 1) / 30.0 - 0.5\n",
        "\n",
        "\n",
        "class DayOfYear(TimeFeature):\n",
        "    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.dayofyear - 1) / 365.0 - 0.5\n",
        "\n",
        "\n",
        "class MonthOfYear(TimeFeature):\n",
        "    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.month - 1) / 11.0 - 0.5\n",
        "\n",
        "\n",
        "class WeekOfYear(TimeFeature):\n",
        "    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
        "\n",
        "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
        "        return (index.isocalendar().week - 1) / 52.0 - 0.5\n"
      ],
      "metadata": {
        "id": "q4LZUE_Y4nfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n",
        "    \"\"\"\n",
        "    Returns a list of time features that will be appropriate for the given frequency string.\n",
        "    Parameters\n",
        "    ----------\n",
        "    freq_str\n",
        "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
        "    \"\"\"\n",
        "\n",
        "    features_by_offsets = {\n",
        "        offsets.YearEnd: [],\n",
        "        offsets.QuarterEnd: [MonthOfYear],\n",
        "        offsets.MonthEnd: [MonthOfYear],\n",
        "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
        "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
        "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
        "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
        "        offsets.Minute: [\n",
        "            MinuteOfHour,\n",
        "            HourOfDay,\n",
        "            DayOfWeek,\n",
        "            DayOfMonth,\n",
        "            DayOfYear,\n",
        "        ],\n",
        "        offsets.Second: [\n",
        "            SecondOfMinute,\n",
        "            MinuteOfHour,\n",
        "            HourOfDay,\n",
        "            DayOfWeek,\n",
        "            DayOfMonth,\n",
        "            DayOfYear,\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    offset = to_offset(freq_str)\n",
        "\n",
        "    for offset_type, feature_classes in features_by_offsets.items():\n",
        "        if isinstance(offset, offset_type):\n",
        "            return [cls() for cls in feature_classes]\n",
        "\n",
        "    supported_freq_msg = f\"\"\"\n",
        "    Unsupported frequency {freq_str}\n",
        "    The following frequencies are supported:\n",
        "        Y   - yearly\n",
        "            alias: A\n",
        "        M   - monthly\n",
        "        W   - weekly\n",
        "        D   - daily\n",
        "        B   - business days\n",
        "        H   - hourly\n",
        "        T   - minutely\n",
        "            alias: min\n",
        "        S   - secondly\n",
        "    \"\"\"\n",
        "    raise RuntimeError(supported_freq_msg)\n",
        "\n",
        "\n",
        "def time_features(dates, freq='h'):\n",
        "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])"
      ],
      "metadata": {
        "id": "yWl9DqLM5t7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metrics"
      ],
      "metadata": {
        "id": "KYfpGfcS6W5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RSE(pred, true):\n",
        "    return np.sqrt(np.sum((true - pred) ** 2)) / np.sqrt(np.sum((true - true.mean()) ** 2))\n",
        "\n",
        "\n",
        "def CORR(pred, true):\n",
        "    u = ((true - true.mean(0)) * (pred - pred.mean(0))).sum(0)\n",
        "    d = np.sqrt(((true - true.mean(0)) ** 2 * (pred - pred.mean(0)) ** 2).sum(0))\n",
        "    return (u / d).mean(-1)\n",
        "\n",
        "\n",
        "def MAE(pred, true):\n",
        "    return np.mean(np.abs(pred - true))\n",
        "\n",
        "\n",
        "def MSE(pred, true):\n",
        "    return np.mean((pred - true) ** 2)\n",
        "\n",
        "\n",
        "def RMSE(pred, true):\n",
        "    return np.sqrt(MSE(pred, true))\n",
        "\n",
        "\n",
        "def MAPE(pred, true):\n",
        "    return np.mean(np.abs(100 * (pred - true) / (true +1e-8)))\n",
        "\n",
        "\n",
        "def MSPE(pred, true):\n",
        "    return np.mean(np.square((pred - true) / (true + 1e-8)))\n",
        "\n",
        "def SMAPE(pred, true):\n",
        "    return np.mean(200 * np.abs(pred - true) / (np.abs(pred) + np.abs(true) + 1e-8))\n",
        "    # return np.mean(200 * np.abs(pred - true) / (pred + true + 1e-8))\n",
        "\n",
        "def ND(pred, true):\n",
        "    return np.mean(np.abs(true - pred)) / np.mean(np.abs(true))\n",
        "\n",
        "def metric(pred, true):\n",
        "    mae = MAE(pred, true)\n",
        "    mse = MSE(pred, true)\n",
        "    rmse = RMSE(pred, true)\n",
        "    mape = MAPE(pred, true)\n",
        "    mspe = MSPE(pred, true)\n",
        "    smape = SMAPE(pred, true)\n",
        "    nd = ND(pred, true)\n",
        "\n",
        "    return mae, mse, rmse, mape, mspe, smape, nd"
      ],
      "metadata": {
        "id": "OXlKfZW854ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading datasets"
      ],
      "metadata": {
        "id": "BcxkNKOxczUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset_ETT_hour(Dataset):\n",
        "    def __init__(self, root_path, flag='train', size=None,\n",
        "                 features='S', data_path='ETTh1.csv',\n",
        "                 target='OT', scale=True, timeenc=0, freq='h',\n",
        "                 percent=100, max_len=-1, train_all=False):\n",
        "        # size [seq_len, label_len, pred_len]\n",
        "        # info\n",
        "        if size == None:\n",
        "            self.seq_len = 24 * 4 * 4\n",
        "            self.label_len = 24 * 4\n",
        "            self.pred_len = 24 * 4\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "        # init\n",
        "        assert flag in ['train', 'test', 'val']\n",
        "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
        "        self.set_type = type_map[flag]\n",
        "\n",
        "        self.percent = percent\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.__read_data__()\n",
        "\n",
        "        self.enc_in = self.data_x.shape[-1]\n",
        "        print(\"self.enc_in = {}\".format(self.enc_in))\n",
        "        print(\"self.data_x = {}\".format(self.data_x.shape))\n",
        "        self.tot_len = len(self.data_x) - self.seq_len - self.pred_len + 1\n",
        "        def __read_data__(self):\n",
        "\n",
        "\n",
        "          self.scaler = StandardScaler(0,1)\n",
        "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
        "                                          self.data_path))\n",
        "\n",
        "        border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len]\n",
        "        border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]\n",
        "        border1 = border1s[self.set_type]\n",
        "        border2 = border2s[self.set_type]\n",
        "\n",
        "        if self.set_type == 0:\n",
        "            border2 = (border2 - self.seq_len) * self.percent // 100 + self.seq_len\n",
        "\n",
        "        if self.features == 'M' or self.features == 'MS':\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features == 'S':\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        if self.scale:\n",
        "            train_data = df_data[border1s[0]:border2s[0]]\n",
        "            self.scaler.fit(train_data.values)\n",
        "            data = self.scaler.transform(df_data.values)\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        df_stamp = df_raw[['date']][border1:border2]\n",
        "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
        "        if self.timeenc == 0:\n",
        "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
        "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
        "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
        "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
        "            data_stamp = df_stamp.drop(['date'], 1).values\n",
        "        elif self.timeenc == 1:\n",
        "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
        "            data_stamp = data_stamp.transpose(1, 0)\n",
        "\n",
        "        self.data_x = data[border1:border2]\n",
        "        self.data_y = data[border1:border2]\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        feat_id = index // self.tot_len\n",
        "        s_begin = index % self.tot_len\n",
        "\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "        seq_x = self.data_x[s_begin:s_end, feat_id:feat_id+1]\n",
        "        seq_y = self.data_y[r_begin:r_end, feat_id:feat_id+1]\n",
        "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
        "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
        "\n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.data_x) - self.seq_len - self.pred_len + 1) * self.enc_in\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return self.scaler.inverse_transform(data)"
      ],
      "metadata": {
        "id": "sW2GKh8acy12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Klm28dXAdDQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset_ETT_minute(Dataset):\n",
        "    def __init__(self, root_path, flag='train', size=None,\n",
        "                 features='S', data_path='ETTm1.csv',\n",
        "                 target='OT', scale=True, timeenc=0, freq='t',\n",
        "                 percent=100, max_len=-1, train_all=False):\n",
        "        # size [seq_len, label_len, pred_len]\n",
        "        # info\n",
        "        if size == None:\n",
        "            self.seq_len = 24 * 4 * 4\n",
        "            self.label_len = 24 * 4\n",
        "            self.pred_len = 24 * 4\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "        # init\n",
        "        assert flag in ['train', 'test', 'val']\n",
        "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
        "        self.set_type = type_map[flag]\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "        self.percent = percent\n",
        "\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.__read_data__()\n",
        "\n",
        "        self.enc_in = self.data_x.shape[-1]\n",
        "        self.tot_len = len(self.data_x) - self.seq_len - self.pred_len + 1\n",
        "\n",
        "    def __read_data__(self):\n",
        "        self.scaler = StandardScaler(0,1)\n",
        "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
        "                                          self.data_path))\n",
        "\n",
        "        border1s = [0, 12 * 30 * 24 * 4 - self.seq_len, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4 - self.seq_len]\n",
        "        border2s = [12 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 8 * 30 * 24 * 4]\n",
        "        border1 = border1s[self.set_type]\n",
        "        border2 = border2s[self.set_type]\n",
        "        if self.set_type == 0:\n",
        "            border2 = (border2 - self.seq_len) * self.percent // 100 + self.seq_len\n",
        "\n",
        "        if self.features == 'M' or self.features == 'MS':\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features == 'S':\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        if self.scale:\n",
        "            train_data = df_data[border1s[0]:border2s[0]]\n",
        "            self.scaler.fit(train_data.values)\n",
        "            data = self.scaler.transform(df_data.values)\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        df_stamp = df_raw[['date']][border1:border2]\n",
        "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
        "        if self.timeenc == 0:\n",
        "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
        "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
        "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
        "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
        "            df_stamp['minute'] = df_stamp.date.apply(lambda row: row.minute, 1)\n",
        "            df_stamp['minute'] = df_stamp.minute.map(lambda x: x // 15)\n",
        "            data_stamp = df_stamp.drop(['date'], 1).values\n",
        "        elif self.timeenc == 1:\n",
        "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
        "            data_stamp = data_stamp.transpose(1, 0)\n",
        "\n",
        "        self.data_x = data[border1:border2]\n",
        "        self.data_y = data[border1:border2]\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        feat_id = index // self.tot_len\n",
        "        s_begin = index % self.tot_len\n",
        "\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "        seq_x = self.data_x[s_begin:s_end, feat_id:feat_id+1]\n",
        "        seq_y = self.data_y[r_begin:r_end, feat_id:feat_id+1]\n",
        "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
        "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
        "\n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.data_x) - self.seq_len - self.pred_len + 1) * self.enc_in\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return self.scaler.inverse_transform(data)"
      ],
      "metadata": {
        "id": "V2D0LeUKdUtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset_Custom(Dataset):\n",
        "    def __init__(self, root_path, flag='train', size=None,\n",
        "                 features='S', data_path='ETTh1.csv',\n",
        "                 target='OT', scale=True, timeenc=0, freq='h',\n",
        "                 percent=10, max_len=-1, train_all=False):\n",
        "        # size [seq_len, label_len, pred_len]\n",
        "        # info\n",
        "        if size == None:\n",
        "            self.seq_len = 24 * 4 * 4\n",
        "            self.label_len = 24 * 4\n",
        "            self.pred_len = 24 * 4\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "        # init\n",
        "        assert flag in ['train', 'test', 'val']\n",
        "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
        "        self.set_type = type_map[flag]\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "        self.percent = percent\n",
        "\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.__read_data__()\n",
        "\n",
        "        self.enc_in = self.data_x.shape[-1]\n",
        "        self.tot_len = len(self.data_x) - self.seq_len - self.pred_len + 1\n",
        "\n",
        "    def __read_data__(self):\n",
        "        self.scaler = StandardScaler(0,1)\n",
        "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
        "                                          self.data_path))\n",
        "\n",
        "        '''\n",
        "        df_raw.columns: ['date', ...(other features), target feature]\n",
        "        '''\n",
        "        cols = list(df_raw.columns)\n",
        "        cols.remove(self.target)\n",
        "        cols.remove('date')\n",
        "        df_raw = df_raw[['date'] + cols + [self.target]]\n",
        "        # print(cols)\n",
        "        num_train = int(len(df_raw) * 0.7)\n",
        "        num_test = int(len(df_raw) * 0.2)\n",
        "        num_vali = len(df_raw) - num_train - num_test\n",
        "        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n",
        "        border2s = [num_train, num_train + num_vali, len(df_raw)]\n",
        "        border1 = border1s[self.set_type]\n",
        "        border2 = border2s[self.set_type]\n",
        "\n",
        "        if self.set_type == 0:\n",
        "            border2 = (border2 - self.seq_len) * self.percent // 100 + self.seq_len\n",
        "\n",
        "        if self.features == 'M' or self.features == 'MS':\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features == 'S':\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        if self.scale:\n",
        "            train_data = df_data[border1s[0]:border2s[0]]\n",
        "            self.scaler.fit(train_data.values)\n",
        "            data = self.scaler.transform(df_data.values)\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        df_stamp = df_raw[['date']][border1:border2]\n",
        "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
        "        if self.timeenc == 0:\n",
        "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
        "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
        "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
        "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
        "            data_stamp = df_stamp.drop(['date'], 1).values\n",
        "        elif self.timeenc == 1:\n",
        "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
        "            data_stamp = data_stamp.transpose(1, 0)\n",
        "\n",
        "        self.data_x = data[border1:border2]\n",
        "        self.data_y = data[border1:border2]\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        feat_id = index // self.tot_len\n",
        "        s_begin = index % self.tot_len\n",
        "\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "        seq_x = self.data_x[s_begin:s_end, feat_id:feat_id+1]\n",
        "        seq_y = self.data_y[r_begin:r_end, feat_id:feat_id+1]\n",
        "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
        "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
        "\n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.data_x) - self.seq_len - self.pred_len + 1) * self.enc_in\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return self.scaler.inverse_transform(data)"
      ],
      "metadata": {
        "id": "W48hP2LwdqxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset_Pred(Dataset):\n",
        "    def __init__(self, root_path, flag='pred', size=None,\n",
        "                 features='S', data_path='ETTh1.csv',\n",
        "                 target='OT', scale=True, inverse=False, timeenc=0, freq='15min', cols=None,\n",
        "                 percent=None, train_all=False):\n",
        "        # size [seq_len, label_len, pred_len]\n",
        "        # info\n",
        "        if size == None:\n",
        "            self.seq_len = 24 * 4 * 4\n",
        "            self.label_len = 24 * 4\n",
        "            self.pred_len = 24 * 4\n",
        "        else:\n",
        "            self.seq_len = size[0]\n",
        "            self.label_len = size[1]\n",
        "            self.pred_len = size[2]\n",
        "        # init\n",
        "        assert flag in ['pred']\n",
        "\n",
        "        self.features = features\n",
        "        self.target = target\n",
        "        self.scale = scale\n",
        "        self.inverse = inverse\n",
        "        self.timeenc = timeenc\n",
        "        self.freq = freq\n",
        "        self.cols = cols\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.__read_data__()\n",
        "\n",
        "    def __read_data__(self):\n",
        "        self.scaler = StandardScaler(0,1)\n",
        "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
        "                                          self.data_path))\n",
        "        '''\n",
        "        df_raw.columns: ['date', ...(other features), target feature]\n",
        "        '''\n",
        "        if self.cols:\n",
        "            cols = self.cols.copy()\n",
        "            cols.remove(self.target)\n",
        "        else:\n",
        "            cols = list(df_raw.columns)\n",
        "            cols.remove(self.target)\n",
        "            cols.remove('date')\n",
        "        df_raw = df_raw[['date'] + cols + [self.target]]\n",
        "        border1 = len(df_raw) - self.seq_len\n",
        "        border2 = len(df_raw)\n",
        "\n",
        "        if self.features == 'M' or self.features == 'MS':\n",
        "            cols_data = df_raw.columns[1:]\n",
        "            df_data = df_raw[cols_data]\n",
        "        elif self.features == 'S':\n",
        "            df_data = df_raw[[self.target]]\n",
        "\n",
        "        if self.scale:\n",
        "            self.scaler.fit(df_data.values)\n",
        "            data = self.scaler.transform(df_data.values)\n",
        "        else:\n",
        "            data = df_data.values\n",
        "\n",
        "        tmp_stamp = df_raw[['date']][border1:border2]\n",
        "        tmp_stamp['date'] = pd.to_datetime(tmp_stamp.date)\n",
        "        pred_dates = pd.date_range(tmp_stamp.date.values[-1], periods=self.pred_len + 1, freq=self.freq)\n",
        "\n",
        "        df_stamp = pd.DataFrame(columns=['date'])\n",
        "        df_stamp.date = list(tmp_stamp.date.values) + list(pred_dates[1:])\n",
        "        if self.timeenc == 0:\n",
        "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
        "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
        "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
        "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
        "            df_stamp['minute'] = df_stamp.date.apply(lambda row: row.minute, 1)\n",
        "            df_stamp['minute'] = df_stamp.minute.map(lambda x: x // 15)\n",
        "            data_stamp = df_stamp.drop(['date'], 1).values\n",
        "        elif self.timeenc == 1:\n",
        "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
        "            data_stamp = data_stamp.transpose(1, 0)\n",
        "\n",
        "        self.data_x = data[border1:border2]\n",
        "        if self.inverse:\n",
        "            self.data_y = df_data.values[border1:border2]\n",
        "        else:\n",
        "            self.data_y = data[border1:border2]\n",
        "        self.data_stamp = data_stamp\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        s_begin = index\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end - self.label_len\n",
        "        r_end = r_begin + self.label_len + self.pred_len\n",
        "\n",
        "        seq_x = self.data_x[s_begin:s_end]\n",
        "        if self.inverse:\n",
        "            seq_y = self.data_x[r_begin:r_begin + self.label_len]\n",
        "        else:\n",
        "            seq_y = self.data_y[r_begin:r_begin + self.label_len]\n",
        "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
        "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
        "\n",
        "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_x) - self.seq_len + 1\n",
        "\n",
        "    def inverse_transform(self, data):\n",
        "        return self.scaler.inverse_transform(data)\n",
        "\n",
        "\n",
        "class Dataset_TSF(Dataset):\n",
        "    def __init__(self, root_path, flag='train', size=None,\n",
        "                 features='S', data_path=None,\n",
        "                 target='OT', scale=True, timeenc=0, freq='Daily',\n",
        "                 percent=10, max_len=-1, train_all=False):\n",
        "\n",
        "        self.train_all = train_all\n",
        "\n",
        "        self.seq_len = size[0]\n",
        "        self.pred_len = size[2]\n",
        "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
        "        self.set_type = type_map[flag]\n",
        "\n",
        "        self.percent = percent\n",
        "        self.max_len = max_len\n",
        "        if self.max_len == -1:\n",
        "            self.max_len = 1e8\n",
        "\n",
        "        self.root_path = root_path\n",
        "        self.data_path = data_path\n",
        "        self.timeseries = self.__read_data__()\n",
        "\n",
        "\n",
        "    def __read_data__(self):\n",
        "        df, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(os.path.join(self.root_path,\n",
        "                                                                                                                              self.data_path))\n",
        "        self.freq = frequency\n",
        "        def dropna(x):\n",
        "            return x[~np.isnan(x)]\n",
        "        timeseries = [dropna(ts).astype(np.float32) for ts in df.series_value]\n",
        "\n",
        "        self.tot_len = 0\n",
        "        self.len_seq = []\n",
        "        self.seq_id = []\n",
        "        for i in range(len(timeseries)):\n",
        "            res_len = max(self.pred_len + self.seq_len - timeseries[i].shape[0], 0)\n",
        "            pad_zeros = np.zeros(res_len)\n",
        "            timeseries[i] = np.hstack([pad_zeros, timeseries[i]])\n",
        "\n",
        "            _len = timeseries[i].shape[0]\n",
        "            train_len = _len-self.pred_len\n",
        "            if self.train_all:\n",
        "                border1s = [0,          0,          train_len-self.seq_len]\n",
        "                border2s = [train_len,  train_len,  _len]\n",
        "            else:\n",
        "                border1s = [0,                          train_len - self.seq_len - self.pred_len, train_len-self.seq_len]\n",
        "                border2s = [train_len - self.pred_len,  train_len,                                _len]\n",
        "            border2s[0] = (border2s[0] - self.seq_len) * self.percent // 100 + self.seq_len\n",
        "            # print(\"_len = {}\".format(_len))\n",
        "\n",
        "            curr_len = border2s[self.set_type] - max(border1s[self.set_type], 0) - self.pred_len - self.seq_len + 1\n",
        "            curr_len = max(0, curr_len)\n",
        "\n",
        "            self.len_seq.append(np.zeros(curr_len) + self.tot_len)\n",
        "            self.seq_id.append(np.zeros(curr_len) + i)\n",
        "            self.tot_len += curr_len\n",
        "\n",
        "        self.len_seq = np.hstack(self.len_seq)\n",
        "        self.seq_id = np.hstack(self.seq_id)\n",
        "\n",
        "        return timeseries\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        len_seq = self.len_seq[index]\n",
        "        seq_id = int(self.seq_id[index])\n",
        "        index = index - int(len_seq)\n",
        "\n",
        "        _len = self.timeseries[seq_id].shape[0]\n",
        "        train_len = _len - self.pred_len\n",
        "        if self.train_all:\n",
        "            border1s = [0,          0,          train_len-self.seq_len]\n",
        "            border2s = [train_len,  train_len,  _len]\n",
        "        else:\n",
        "            border1s = [0,                          train_len - self.seq_len - self.pred_len, train_len-self.seq_len]\n",
        "            border2s = [train_len - self.pred_len,  train_len,                                _len]\n",
        "        border2s[0] = (border2s[0] - self.seq_len) * self.percent // 100 + self.seq_len\n",
        "\n",
        "        s_begin = index + border1s[self.set_type]\n",
        "        s_end = s_begin + self.seq_len\n",
        "        r_begin = s_end\n",
        "        r_end = r_begin + self.pred_len\n",
        "        if self.set_type == 2:\n",
        "            s_end = -self.pred_len\n",
        "\n",
        "        data_x = self.timeseries[seq_id][s_begin:s_end]\n",
        "        data_y = self.timeseries[seq_id][r_begin:r_end]\n",
        "        data_x = np.expand_dims(data_x, axis=-1)\n",
        "        data_y = np.expand_dims(data_y, axis=-1)\n",
        "        # if self.set_type == 2:\n",
        "        #     print(\"data_x.shape = {}, data_y.shape = {}\".format(data_x.shape, data_y.shape))\n",
        "\n",
        "        return data_x, data_y, data_x, data_y\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.set_type == 0:\n",
        "            # return self.tot_len\n",
        "            return min(self.max_len, self.tot_len)\n",
        "        else:\n",
        "            return self.tot_len"
      ],
      "metadata": {
        "id": "2dg-WX1vR8Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = {\n",
        "    'custom': Dataset_Custom,\n",
        "    'tsf_data': Dataset_TSF,\n",
        "    'ett_h': Dataset_ETT_hour,\n",
        "    'ett_m': Dataset_ETT_minute,\n",
        "}\n",
        "\n",
        "\n",
        "def data_provider(args, flag, drop_last_test=True, train_all=False):\n",
        "    Data = data_dict[args.data]\n",
        "    timeenc = 0 if args.embed != 'timeF' else 1\n",
        "    percent = args.percent\n",
        "    max_len = args.max_len\n",
        "\n",
        "    if flag == 'test':\n",
        "        shuffle_flag = False\n",
        "        drop_last = drop_last_test\n",
        "        batch_size = args.batch_size\n",
        "        freq = args.freq\n",
        "    elif flag == 'pred':\n",
        "        shuffle_flag = False\n",
        "        drop_last = False\n",
        "        batch_size = 1\n",
        "        freq = args.freq\n",
        "        Data = Dataset_Pred\n",
        "    elif flag == 'val':\n",
        "        shuffle_flag = True\n",
        "        drop_last = drop_last_test\n",
        "        batch_size = args.batch_size\n",
        "        freq = args.freq\n",
        "    elif flag == 'train':\n",
        "        shuffle_flag = True\n",
        "        drop_last = True\n",
        "        batch_size = args.batch_size\n",
        "        freq = args.freq\n",
        "\n",
        "    data_set = Data(\n",
        "        root_path=args.root_path,\n",
        "        data_path=args.data_path,\n",
        "        flag=flag,\n",
        "        size=[args.seq_len, args.label_len, args.pred_len],\n",
        "        features=args.features,\n",
        "        target=args.target,\n",
        "        timeenc=timeenc,\n",
        "        freq=freq,\n",
        "        percent=percent,\n",
        "        max_len=max_len,\n",
        "        train_all=train_all\n",
        "    )\n",
        "    print(flag, len(data_set))\n",
        "    data_loader = DataLoader(\n",
        "        data_set,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle_flag,\n",
        "        num_workers=args.num_workers,\n",
        "        drop_last=drop_last)\n",
        "    return data_set, data_loader"
      ],
      "metadata": {
        "id": "Dh8LI8l8DZIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "QRUnKauEQmiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fix_seed = 2021\n",
        "random.seed(fix_seed)\n",
        "torch.manual_seed(fix_seed)\n",
        "np.random.seed(fix_seed)\n"
      ],
      "metadata": {
        "id": "LnC5ONrOd0i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gpt_traceback"
      ],
      "metadata": {
        "id": "mI2HZl1SWP_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gpt_traceback"
      ],
      "metadata": {
        "id": "wMTjYwIPWVI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description='GPTNeoX4TS')\n",
        "\n",
        "parser.add_argument('--checkpoints', type=str, default='./checkpoints/')\n",
        "\n",
        "parser.add_argument('--root_path', type=str, default='./dataset/traffic/')\n",
        "parser.add_argument('--data_path', type=str, default='traffic.csv')\n",
        "parser.add_argument('--data', type=str, default='custom')\n",
        "parser.add_argument('--features', type=str, default='M')\n",
        "parser.add_argument('--freq', type=int, default=1)\n",
        "parser.add_argument('--target', type=str, default='OT')\n",
        "parser.add_argument('--embed', type=str, default='timeF')\n",
        "parser.add_argument('--percent', type=int, default=10)\n",
        "\n",
        "parser.add_argument('--seq_len', type=int, default=512)\n",
        "parser.add_argument('--pred_len', type=int, default=96)\n",
        "parser.add_argument('--label_len', type=int, default=48)\n",
        "\n",
        "parser.add_argument('--decay_fac', type=float, default=0.75)\n",
        "parser.add_argument('--learning_rate', type=float, default=0.0001)\n",
        "parser.add_argument('--batch_size', type=int, default=512)\n",
        "parser.add_argument('--num_workers', type=int, default=10)\n",
        "parser.add_argument('--train_epochs', type=int, default=10)\n",
        "parser.add_argument('--lradj', type=str, default='type1')\n",
        "parser.add_argument('--patience', type=int, default=3)\n",
        "\n",
        "parser.add_argument('--gpt_layers', type=int, default=3)\n",
        "parser.add_argument('--is_gpt', type=int, default=1)\n",
        "parser.add_argument('--e_layers', type=int, default=3)\n",
        "parser.add_argument('--d_model', type=int, default=768)\n",
        "parser.add_argument('--n_heads', type=int, default=16)\n",
        "parser.add_argument('--d_ff', type=int, default=512)\n",
        "parser.add_argument('--dropout', type=float, default=0.2)\n",
        "parser.add_argument('--enc_in', type=int, default=862)\n",
        "parser.add_argument('--c_out', type=int, default=862)\n",
        "parser.add_argument('--patch_size', type=int, default=16)\n",
        "parser.add_argument('--kernel_size', type=int, default=25)\n",
        "\n",
        "parser.add_argument('--loss_func', type=str, default='mse')\n",
        "parser.add_argument('--pretrain', type=int, default=1)\n",
        "parser.add_argument('--freeze', type=int, default=1)\n",
        "parser.add_argument('--model', type=str, default='model')\n",
        "parser.add_argument('--stride', type=int, default=8)\n",
        "parser.add_argument('--max_len', type=int, default=-1)\n",
        "parser.add_argument('--hid_dim', type=int, default=16)\n",
        "parser.add_argument('--tmax', type=int, default=10)\n",
        "\n",
        "parser.add_argument('--itr', type=int, default=3)\n",
        "parser.add_argument('--cos', type=int, default=0)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQjApmjwQqAY",
        "outputId": "ed5d059f-7898-40ae-a9fd-f95b4b1699cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--cos'], dest='cos', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, required=False, help=None, metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args, unknown = parser.parse_known_args()"
      ],
      "metadata": {
        "id": "YnnYzRMNYbV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some arguments weren't parsed correctly, have to put in the unknown but idk what's missing."
      ],
      "metadata": {
        "id": "achmLpk-d-Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEASONALITY_MAP = {\n",
        "   \"minutely\": 1440,\n",
        "   \"10_minutes\": 144,\n",
        "   \"half_hourly\": 48,\n",
        "   \"hourly\": 24,\n",
        "   \"daily\": 7,\n",
        "   \"weekly\": 1,\n",
        "   \"monthly\": 12,\n",
        "   \"quarterly\": 4,\n",
        "   \"yearly\": 1\n",
        "}\n",
        "\n",
        "mses = []\n",
        "maes = []\n",
        "\n",
        "for ii in range(args.itr):\n",
        "\n",
        "    setting = 'sl{}_ll{}_pl{}_dm{}_nh{}_el{}_gl{}_df{}_eb{}_itr{}'.format(336, args.label_len, args.pred_len,\n",
        "                                                                    args.d_model, args.n_heads, args.e_layers, args.gpt_layers,\n",
        "                                                                    args.d_ff, args.embed, ii)\n",
        "    path = os.path.join(args.checkpoints, setting)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "    if args.freq == 0:\n",
        "        args.freq = 'h'\n",
        "\n",
        "    train_data, train_loader = data_provider(args, 'train')\n",
        "    vali_data, vali_loader = data_provider(args, 'val')\n",
        "    test_data, test_loader = data_provider(args, 'test')\n",
        "\n",
        "    if args.freq != 'h':\n",
        "        args.freq = SEASONALITY_MAP[test_data.freq]\n",
        "        print(\"freq = {}\".format(args.freq))\n",
        "\n",
        "    device = torch.device('cuda:0')\n",
        "\n",
        "    time_now = time.time()\n",
        "    train_steps = len(train_loader)\n",
        "\n",
        "\n",
        "    model = GPTNeoX4TS(args, device)\n",
        "    # mse, mae = test(model, test_data, test_loader, args, device, ii)\n",
        "\n",
        "    params = model.parameters()\n",
        "    model_optim = torch.optim.Adam(params, lr=args.learning_rate)\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=args.patience, verbose=True)\n",
        "    if args.loss_func == 'mse':\n",
        "        criterion = nn.MSELoss()\n",
        "    elif args.loss_func == 'smape':\n",
        "        class SMAPE(nn.Module):\n",
        "            def __init__(self):\n",
        "                super(SMAPE, self).__init__()\n",
        "            def forward(self, pred, true):\n",
        "                return torch.mean(200 * torch.abs(pred - true) / (torch.abs(pred) + torch.abs(true) + 1e-8))\n",
        "        criterion = SMAPE()\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=args.tmax, eta_min=1e-8)\n",
        "\n",
        "    for epoch in range(args.train_epochs):\n",
        "\n",
        "        iter_count = 0\n",
        "        train_loss = []\n",
        "        epoch_time = time.time()\n",
        "        for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in tqdm(enumerate(train_loader)):\n",
        "\n",
        "            iter_count += 1\n",
        "            model_optim.zero_grad()\n",
        "            batch_x = batch_x.float().to(device)\n",
        "\n",
        "            batch_y = batch_y.float().to(device)\n",
        "            batch_x_mark = batch_x_mark.float().to(device)\n",
        "            batch_y_mark = batch_y_mark.float().to(device)\n",
        "\n",
        "            outputs = model(batch_x, ii)\n",
        "\n",
        "            outputs = outputs[:, -args.pred_len:, :]\n",
        "            batch_y = batch_y[:, -args.pred_len:, :].to(device)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            train_loss.append(loss.item())\n",
        "\n",
        "            if (i + 1) % 1000 == 0:\n",
        "                print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
        "                speed = (time.time() - time_now) / iter_count\n",
        "                left_time = speed * ((args.train_epochs - epoch) * train_steps - i)\n",
        "                print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
        "                iter_count = 0\n",
        "                time_now = time.time()\n",
        "            loss.backward()\n",
        "            model_optim.step()\n",
        "\n",
        "\n",
        "        print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
        "\n",
        "        train_loss = np.average(train_loss)\n",
        "        vali_loss = vali(model, vali_data, vali_loader, criterion, args, device, ii)\n",
        "        # test_loss = vali(model, test_data, test_loader, criterion, args, device, ii)\n",
        "        # print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f}, Test Loss: {4:.7f}\".format(\n",
        "        #     epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
        "        print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f}\".format(\n",
        "            epoch + 1, train_steps, train_loss, vali_loss))\n",
        "\n",
        "        if args.cos:\n",
        "            scheduler.step()\n",
        "            print(\"lr = {:.10f}\".format(model_optim.param_groups[0]['lr']))\n",
        "        else:\n",
        "            adjust_learning_rate(model_optim, epoch + 1, args)\n",
        "        early_stopping(vali_loss, model, path)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    best_model_path = path + '/' + 'checkpoint.pth'\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    print(\"------------------------------------\")\n",
        "    mse, mae = test(model, test_data, test_loader, args, device, ii)\n",
        "    mses.append(mse)\n",
        "    maes.append(mae)\n",
        "\n",
        "mses = np.array(mses)\n",
        "maes = np.array(maes)\n",
        "print(\"mse_mean = {:.4f}, mse_std = {:.4f}\".format(np.mean(mses), np.std(mses)))\n",
        "print(\"mae_mean = {:.4f}, mae_std = {:.4f}\".format(np.mean(maes), np.std(maes)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "2CfQmr5i9No6",
        "outputId": "c8458335-75af-44f4-ac0a-ff97194f0787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-543ac1bacd7c>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'h'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mvali_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvali_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-67-d9d714438163>\u001b[0m in \u001b[0;36mdata_provider\u001b[0;34m(args, flag, drop_last_test, train_all)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     data_set = Data(\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mroot_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-122742834a30>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_path, flag, size, features, data_path, target, scale, timeenc, freq, percent, max_len, train_all)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__read_data__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-122742834a30>\u001b[0m in \u001b[0;36m__read_data__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__read_data__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         df_raw = pd.read_csv(os.path.join(self.root_path,\n\u001b[0m\u001b[1;32m     38\u001b[0m                                           self.data_path))\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/traffic/traffic.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem : The StandardScaler()  requires mean and std values, they didnt include it while defining the dataloader, I've defined it in each data loader but it still didnt run."
      ],
      "metadata": {
        "id": "mqCyM9SYdbzt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuration from scripts, install terminal"
      ],
      "metadata": {
        "id": "5lAb8atfdQ6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-xterm\n",
        "%load_ext colabxterm"
      ],
      "metadata": {
        "id": "C08MdCGSdB2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "or just using %%shell, don't know if it'll work."
      ],
      "metadata": {
        "id": "XE8qUcrvkY8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "export CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "seq_len=336\n",
        "model=GPTNeoX4TS\n",
        "\n",
        "for percent in 5 10\n",
        "do\n",
        "    for pred_len in 96 192 336 720\n",
        "    do\n",
        "        for lr in 0.001 0.000005\n",
        "        do\n",
        "\n",
        "                --root_path ./datasets/ETT-small/ \\\n",
        "                --data_path ETTh1.csv \\\n",
        "\n",
        "    --data ett_h \\\n",
        "    --seq_len $seq_len \\\n",
        "    --label_len 168 \\\n",
        "    --pred_len $pred_len \\\n",
        "    --batch_size 256 \\\n",
        "    --lradj type4 \\\n",
        "    --learning_rate $lr \\\n",
        "    --train_epochs 10 \\\n",
        "    --decay_fac 0.5 \\\n",
        "    --d_model 768 \\\n",
        "    --n_heads 4 \\\n",
        "    --d_ff 768 \\\n",
        "    --dropout 0.3 \\\n",
        "    --enc_in 7 \\\n",
        "    --c_out 7 \\\n",
        "    --freq 0 \\\n",
        "    --patch_size 16 \\\n",
        "    --stride 8 \\\n",
        "    --percent $percent \\\n",
        "    --gpt_layer 6 \\\n",
        "    --itr 3 \\\n",
        "    --model $model \\\n",
        "    --tmax 20 \\\n",
        "    --cos 1 \\\n",
        "    --is_gpt 1\n",
        "\n",
        "done\n",
        "done\n",
        "done"
      ],
      "metadata": {
        "id": "hum0vHZRkYlI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}